{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# \"Big\" Data Science\n",
    "\n",
    "### Guest Lecture in Introduction to Data Science at TAU / Prof. Saharon Rosset\n",
    "### Giora Simchoni\n",
    "### June 4th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Small? What is Big?\n",
    "\n",
    "These definitions are constantly changing.\n",
    "\n",
    "(1) \"Everything processed in Excel is small data.\" ([Rufus Pollock, The Guardian](https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution))\n",
    "\n",
    "(2) \"[Big Data] is data so large it does not fit in main memory\" (Leskovec et al., Mining of Massive Datasets)\n",
    "\n",
    "Or maybe we should define the size of our data according how easy it is to process and understand it?\n",
    "\n",
    "\n",
    "(3) \"[Small Data is] data that has small enough size for human comprehension.\" ([jWork.ORG](jWork.ORG))\n",
    "\n",
    "(4) \"data sets that are too large or complex for traditional data-processing application software to adequately deal with\" ([Wikipedia](https://en.wikipedia.org/wiki/Big_data))\n",
    "\n",
    "We'll talk about solutions to (3) and (4) today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When data is too big to fit in RAM...\n",
    "\n",
    "### You want Regression? Deep Learning? Try computing an average!\n",
    "\n",
    "### Distributed File System\n",
    "* a new form of file system\n",
    "* data is distributed over computing clusters (a collection of hundreds to thousands of \"computer nodes\")\n",
    "\n",
    "### MapReduce\n",
    "* a new software methodology which knows how to \"speak\" to data on DFS\n",
    "* from computing `SELECT Country, COUNT(*) FROM table_name GROUP BY Country` to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed File System (DFS)\n",
    "\n",
    "* Typically: [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html), another open source project by [Apache](https://www.apache.org/)\n",
    "\n",
    "* \"Computers\" (nodes) are stacked in racks of 8-64 size each.\n",
    "\n",
    "* Nodes on the same rack are connected, and racks are connected by a network or \"switch\"\n",
    "\n",
    "<img src = \"images/computer_cluster.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probably the only time this image is called for\n",
    "\n",
    "<img src = \"images/actual_cluster.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fact: Computers Fail\n",
    "* If a node's disk has 1 in a million chance of failing the next minute, what is the chance of (at least one) failure on a cluster of 10000 nodes?\n",
    "\n",
    "* Solution: chunk your data into say 64MB-size chunks, and replicate on say 3 nodes, on different racks\n",
    "\n",
    "<img src = \"images/computer_cluster_files.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce\n",
    "\n",
    "* Typically: [Hadoop](https://hadoop.apache.org/) by Apache\n",
    "* Three main features:\n",
    "    1. Parallel computations, exploiting the cluster\n",
    "    2. Tolerant to hardware failures\n",
    "    3. All you need is a Map function and a Reduce function\n",
    "    \n",
    "## The Mapper\n",
    "A function which will take one or more file chunks and sum them up to (key, value) mapping. The mappings from a few mappers are then grouped to (key, [values]) mapping, and sorted by key.\n",
    "\n",
    "## The Reducer\n",
    "A function which will take one of those grouped mapping and aggregate them in some way. A reducer typically handles one key at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Classic MapReduce Example: Word Count\n",
    "\n",
    "<img src = \"images/word_count1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Count: The Mapper\n",
    "\n",
    "<img src = \"images/word_count2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Count: MapReduce then groups and sorts, regardless of input:\n",
    "\n",
    "<img src = \"images/word_count3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Count: The Reducer\n",
    "\n",
    "<img src = \"images/word_count4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Functional View\n",
    "\n",
    "<img src = \"images/mapreduce_highlevel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Architectural View\n",
    "\n",
    "<img src = \"images/mapreduce_architecture.png\">\n",
    "\n",
    "Question: why would we prefer a smaller number of Reducer workers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce: Failure is an Option\n",
    "\n",
    "What happens when...\n",
    "\n",
    "* The Master is down?\n",
    "* A Map worker is down?\n",
    "* A Reduce worker is down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce: In Action (almost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mapper(text):\n",
    "    list_of_key_values = []\n",
    "    for word in text.split(' '):\n",
    "        if word != '':\n",
    "            list_of_key_values.append((word, 1))\n",
    "    return list_of_key_values\n",
    "\n",
    "def reducer(single_key_values):\n",
    "    key, values = single_key_values\n",
    "    return key, sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 1),\n",
       " ('Captain!', 1),\n",
       " ('my', 1),\n",
       " ('Captain!', 1),\n",
       " ('our', 1),\n",
       " ('fearful', 1),\n",
       " ('trip', 1),\n",
       " ('is', 1),\n",
       " ('done,', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper('O Captain! my Captain! our fearful trip is done,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1),\n",
       " ('ship', 1),\n",
       " ('has', 1),\n",
       " ('weather’d', 1),\n",
       " ('every', 1),\n",
       " ('rack,', 1),\n",
       " ('the', 1),\n",
       " ('prize', 1),\n",
       " ('we', 1),\n",
       " ('sought', 1),\n",
       " ('is', 1),\n",
       " ('won,', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper('The ship has weather’d every rack, the prize we sought is won,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1),\n",
       " ('port', 1),\n",
       " ('is', 1),\n",
       " ('near,', 1),\n",
       " ('the', 1),\n",
       " ('bells', 1),\n",
       " ('I', 1),\n",
       " ('hear,', 1),\n",
       " ('the', 1),\n",
       " ('people', 1),\n",
       " ('all', 1),\n",
       " ('exulting', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper('The port is near, the bells I hear, the people all exulting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Captain', 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember Hadoop will group map output for you!\n",
    "# remember single key for single reducer, though single reducer can handle a few keys\n",
    "reducer(('Captain', [1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running a Hadoop Job - Live Demo\n",
    "\n",
    "```\n",
    "bash-4.1$ bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-file /home/mapper.py -mapper /home/mapper.py \\\n",
    "-file /home/reducer.py -reducer /home/reducer.py \\\n",
    "-input shakespeare -output output -numReduceTasks 4\n",
    "\n",
    "packageJobJar: [/home/mapper.py, /home/reducer.py, /tmp/hadoop-unjar2660950065718379808/] [] /tmp/streamjob5124415768465865698.jar tmpDir=null\n",
    "19/05/30 10:50:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/05/30 10:50:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/05/30 10:50:11 INFO mapred.FileInputFormat: Total input paths to process : 10\n",
    "19/05/30 10:50:11 INFO mapreduce.JobSubmitter: number of splits:10\n",
    "19/05/30 10:50:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559226271713_0005\n",
    "19/05/30 10:50:12 INFO impl.YarnClientImpl: Submitted application application_1559226271713_0005\n",
    "19/05/30 10:50:12 INFO mapreduce.Job: The url to track the job: http://2cd2e5b21407:8088/proxy/application_1559226271713_0005/\n",
    "19/05/30 10:50:12 INFO mapreduce.Job: Running job: job_1559226271713_0005\n",
    "19/05/30 10:50:18 INFO mapreduce.Job: Job job_1559226271713_0005 running in uber mode : false\n",
    "19/05/30 10:50:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "19/05/30 10:50:35 INFO mapreduce.Job:  map 60% reduce 0%\n",
    "19/05/30 10:50:52 INFO mapreduce.Job:  map 90% reduce 0%\n",
    "19/05/30 10:50:53 INFO mapreduce.Job:  map 90% reduce 15%\n",
    "19/05/30 10:50:54 INFO mapreduce.Job:  map 90% reduce 23%\n",
    "19/05/30 10:50:59 INFO mapreduce.Job:  map 100% reduce 32%\n",
    "19/05/30 10:51:00 INFO mapreduce.Job:  map 100% reduce 50%\n",
    "19/05/30 10:51:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "19/05/30 10:51:03 INFO mapreduce.Job: Job job_1559226271713_0005 completed successfully\n",
    "\n",
    "bash-4.1$ bin/hadoop fs -ls output\n",
    "Found 5 items\n",
    "-rw-r--r--   1 root supergroup          0 2019-05-30 10:51 output3/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup      78211 2019-05-30 10:51 output3/part-00000\n",
    "-rw-r--r--   1 root supergroup      77696 2019-05-30 10:51 output3/part-00001\n",
    "-rw-r--r--   1 root supergroup      77111 2019-05-30 10:51 output3/part-00002\n",
    "-rw-r--r--   1 root supergroup      78458 2019-05-30 10:51 output3/part-00003\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce: Plausible Exercise\n",
    "\n",
    "Facebook has 2.5 billion users.\n",
    "\n",
    "Suppose (and we're REALLY simpifying things here) files in HDFS contain each a list of integers, which are the number of friends each user has.\n",
    "\n",
    "Write a Mapper and a Reducer to compute the maximum number of friends a user has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mapper(list_of_ints):\n",
    "    ### Your code here\n",
    "    pass\n",
    "    \n",
    "def reducer(single_key_values):\n",
    "    ### Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mapper(list_of_ints):\n",
    "    list_of_key_values = [('whatever', max(list_of_ints))]\n",
    "    return list_of_key_values\n",
    "    \n",
    "def reducer(single_key_values):\n",
    "    key, values = single_key_values\n",
    "    return key, max(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce: Another Plausible Exercise\n",
    "\n",
    "Which of the following will NOT be \"naturally-fitting\" to the MapReduce methodology? More than 1 answer is correct.\n",
    "\n",
    "a. Getting the average annual income by country from a huge table containing each adult person in the world and his/her annual income\n",
    "\n",
    "b. Getting the median annual income by country from a huge table containing each adult person in the world and his/her annual income\n",
    "\n",
    "c. Multiplying a distributed huge 1-trillion x 1-trillion matrix with a 1-trillion long vector\n",
    "\n",
    "d. Fitting a Random Forests model with 1000 trees to a medium-size dataset which can fit in a single node RAM\n",
    "\n",
    "e. Fitting Gradient Boosted Trees model with 1000 trees to a medium-size dataset which can fit in a single node RAM\n",
    "\n",
    "f. Getting the average income by settlement from a file containing each person in Israel, his/her city and income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But surely you don't need to write a Mapper and a Reducer each time you want a simple average, let alone fit a Random Forest model?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop Ecosystem\n",
    "\n",
    "<img src = \"images/hadoop_ecosystem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pig Script Example\n",
    "\n",
    "```\n",
    "A = LOAD 'data' AS (f1:int,f2:int,f3:int);\n",
    "\n",
    "DUMP A;\n",
    "(1,2,3)\n",
    "(4,2,1)\n",
    "(8,3,4)\n",
    "(4,3,3)\n",
    "(7,2,5)\n",
    "(8,4,3)\n",
    "\n",
    "B = GROUP A BY f1;\n",
    "\n",
    "DUMP B;\n",
    "(1,{(1,2,3)})\n",
    "(4,{(4,2,1),(4,3,3)})\n",
    "(7,{(7,2,5)})\n",
    "(8,{(8,3,4),(8,4,3)})\n",
    "\n",
    "X = FOREACH B GENERATE COUNT(A);\n",
    "\n",
    "DUMP X;\n",
    "(1L)\n",
    "(2L)\n",
    "(1L)\n",
    "(2L)\n",
    "```\n",
    "\n",
    "# Hive Script Example\n",
    "\n",
    "```\n",
    "SELECT COUNT(*) FROM table2;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "* [Spark](https://spark.apache.org/) by Apache is a \"unified analytics engine for large-scale data processing.\"\n",
    "* Write (single) programs in Java, Scala, Python (PySpark), R (SparkR), and SQL.\n",
    "* Works not only with Hadoop data! Also as a standalone (you can install Spark on your laptop), on streaming data, Kubernetes and more\n",
    "* Especially suitable for Machine Learning\n",
    "* But for Deep Learning you're probably already using [TensorFlow](https://www.tensorflow.org/) by Google or [Keras](https://keras.io/) on top of it\n",
    "* And for parallel data manipulations utilizing a multicore CPU I'd also look at [Dask](https://dask.org/) by Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark: Greatest advantage - In-memory computation\n",
    "\n",
    "* Hadoop: Tasks are distributed among the nodes of a cluster, which in turn load/save data on disk\n",
    "* Spark: Saves and loads the data in and from the RAM rather than from the disk\n",
    "\n",
    "<img src = \"images/spark_vs_hadoop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark: RDD and DataFrame\n",
    "\n",
    "Every framework has its basic data objects or structures:\n",
    "\n",
    "* Basic R has: vector, list, data.frame, matrix, factor\n",
    "* Basic Python has: list, set, dictionary, tuple\n",
    "* Tidyverse (R) has: tibble\n",
    "* Pandas (Python) has: Series, DataFrame\n",
    "* Spark has: RDD, DataFrame, DataSet (only in Scala/Java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD (Resilient Distributed Dataset)\n",
    "\n",
    "* Collection of elements (lines of text, tuples (rows) of table), that can be divided across multiple nodes in a cluster to run parallel processing\n",
    "* \"Resilient\" = can automatically recover from node failures\n",
    "* Immutable - you can't change a RDD, you can only **transform** it into another RDD (Map) or perform an **action** on it in some way (Reduce)\n",
    "* Lazy Evaluation - nothing happens when you transform a RDD! Not until you perform an action, in which case Spark remembers all the transformations which have led your data to where it is (in a DAG), and moves your data along these \"pipes\" in an optimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "rdd.map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "rdd.flatMap(lambda x: range(1, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "rdd.filter(lambda x: x < 4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4, 5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey\n",
    "rdd.sortBy(lambda x: x, ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Basic Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "rdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5015"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold\n",
    "rdd.fold(1000, lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count, sum, mean, min, max, stdev, variance...\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Functional Programming (Chaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increase each value by 10 and sum\n",
    "rdd.map(lambda x: x + 10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 5)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by remainder of division by 3,\n",
    "# sum each group's values,\n",
    "# sort RDD by remainder\n",
    "# and filter by remainder smaller than 2\n",
    "rdd.groupBy(lambda x: x % 3) \\\n",
    "    .mapValues(sum) \\\n",
    "    .sortByKey() \\\n",
    "    .filter(lambda x: x[0] < 2) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shakespeare = sc.textFile('/home/jovyan/hadoop_example/shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44640"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wc = shakespeare \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6788),\n",
       " ('and', 5113),\n",
       " ('I', 5036),\n",
       " ('to', 4298),\n",
       " ('of', 3930),\n",
       " ('a', 3160),\n",
       " ('my', 2829),\n",
       " ('in', 2532),\n",
       " ('you', 2490),\n",
       " ('is', 2101)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.top(10, key = lambda t: t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames\n",
    "\n",
    "* (inspired by Pandas which is inspired by R)\n",
    "* Collection of `Row`s under named `Column`s\n",
    "* Especially suitable for tabular data\n",
    "* Syntax very similar to Pandas\n",
    "* Like RDD: Resilient, Distirbuted, Lazy evaluated, Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.createDataFrame(data=[(1, 'A'), (2, 'B'), (3, 'C')], schema=['id', 'name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "|  3|   C|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Basic DataFrame Manipulations (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saheart = spark.read.csv('SAheart.csv', header = True, inferSchema = True)\n",
    "saheart.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row.names',\n",
       " 'sbp',\n",
       " 'tobacco',\n",
       " 'ldl',\n",
       " 'adiposity',\n",
       " 'famhist',\n",
       " 'typea',\n",
       " 'obesity',\n",
       " 'alcohol',\n",
       " 'age',\n",
       " 'chd']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saheart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "saheart = saheart.drop('row.names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+\n",
      "|sbp|tobacco| ldl|adiposity|famhist|typea|obesity|alcohol|age|chd|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+\n",
      "|160|   12.0|5.73|    23.11|Present|   49|   25.3|   97.2| 52|  1|\n",
      "|144|   0.01|4.41|    28.61| Absent|   55|  28.87|   2.06| 63|  1|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saheart.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Basic DataFrame Manipulations (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|famhist|\n",
      "+-------+\n",
      "|Present|\n",
      "| Absent|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saheart.select('famhist').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|famhist|           avg(chd)|\n",
      "+-------+-------------------+\n",
      "|Present|                0.5|\n",
      "| Absent|0.23703703703703705|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saheart.groupby('famhist').agg({'chd': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.051619568611913025"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saheart.stat.corr('obesity', 'alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Processing DataFrame for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "train, test = saheart.randomSplit([0.8, 0.2])\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+\n",
      "|sbp|tobacco| ldl|adiposity|famhist|typea|obesity|alcohol|age|chd|            features|label|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+\n",
      "|102|    0.4|3.41|    17.22|Present|   56|  23.59|   2.06| 39|  1|[102.0,0.4,3.41,1...|  1.0|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "formula = RFormula(formula='chd ~ .', featuresCol='features', labelCol='label')\n",
    "\n",
    "fit = formula.fit(train)\n",
    "train_df = fit.transform(train)\n",
    "train_df.show(1) # notice the two extra columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_df = fit.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Logistic Regression (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.008408247604829246,0.0819338712417374,0.1844407014226751,0.031730852428542095,-0.9351098151988192,0.03907230665069162,-0.09409933142418063,0.0016123733510384239,0.028466557617748908]\n",
      "Intercept: -4.259414174733452\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.0)\n",
    "\n",
    "lrModel = lr.fit(train_df)\n",
    "\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "|sbp|tobacco| ldl|adiposity|famhist|typea|obesity|alcohol|age|chd|            features|label|       rawPrediction|         probability|prediction|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "|101|   0.48|7.26|     13.0| Absent|   50|  19.82|   5.19| 16|  0|[101.0,0.48,7.26,...|  0.0|[2.00202242646562...|[0.88100925630563...|       0.0|\n",
      "|108|    0.0|1.43|    26.26| Absent|   42|  19.38|    0.0| 16|  0|[108.0,0.0,1.43,2...|  0.0|[2.91657410259036...|[0.94865969878719...|       0.0|\n",
      "+---+-------+----+---------+-------+-----+-------+-------+---+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df = lrModel.transform(test_df)\n",
    "pred_df.show(2) # notice the extra columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Logistic Regression (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "# assuming data is small\n",
    "import numpy as np\n",
    "y_pred = np.array([int(row.prediction) for row in pred_df.select('prediction').collect()])\n",
    "y_true = np.array([int(row.label) for row in pred_df.select('label').collect()])\n",
    "print('Test accuracy: %.2f' % np.mean(y_pred == y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.83\n"
     ]
    }
   ],
   "source": [
    "# for AUC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "auc = evaluator.evaluate(pred_df)\n",
    "print('Test AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=1000)\n",
    "\n",
    "rfModel = rf.fit(train_df)\n",
    "\n",
    "pred_df = rfModel.transform(test_df)\n",
    "\n",
    "# when data isn't so small, get accuracy with MulticlassClassificationEvaluator, but beware of the automatic threshold\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(pred_df)\n",
    "print('Test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.79\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "auc = evaluator.evaluate(pred_df)\n",
    "print('Test AUC: %.2f' % auc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
